#  Fynd Feedback Evaluation ‚Äî LLM Review Rating Intelligence System

This project evaluates how well Large Language Models (LLMs) predict
star-ratings from customer review text, and how different prompt designs
impact accuracy, reasoning quality, and behavioural consistency.

The work is structured in two stages:

1Ô∏è‚É£ Task 1 ‚Äî Prompt behaviour exploration & structured JSON evaluation  
2Ô∏è‚É£ Task 2 ‚Äî Feedback evaluation API with accuracy benchmarking

The goal is to understand whether the model:

‚Ä¢ interprets tone & sentiment correctly  
‚Ä¢ assigns logical star ratings  
‚Ä¢ explains reasoning meaningfully  
‚Ä¢ behaves differently across prompt versions  

This project combines experimentation, prompt engineering,
backend logging, and evaluation analytics.

---

## Task 1 ‚Äî Prompt Behaviour & Structured Output Evaluation

Task-1 focused on **exploring LLM behaviour**, not accuracy.

The objective was to test:

‚úî how Gemini responds to different prompts  
‚úî whether output remains structured and parsable  
‚úî consistency of reasoning explanations  
‚úî stability of JSON format across samples  

Multiple prompt versions were tested to observe:

‚Ä¢ rating judgement tendencies  
‚Ä¢ tone interpretation ability  
‚Ä¢ handling of mixed sentiment reviews  
‚Ä¢ variability across responses  

The notebook collects:

‚úî review text  
‚úî predicted star rating  
‚úî explanation reasoning  
‚úî JSON validation status  

This helped understand:

‚Ä¢ when the model follows instructions  
‚Ä¢ when it deviates from structure  
‚Ä¢ how prompt wording influences response behaviour  

 Notebook  
`/notebooks/fynd_prompt_eval.ipynb`

This stage served as the **research & experimentation foundation**
for Task-2 benchmarking.

---

## Task 2 ‚Äî Feedback Evaluation API & Accuracy Benchmarking

Task-2 converts experimentation into a measurable evaluation system.

A FastAPI backend was developed to:

‚úî submit evaluation entries  
‚úî log predictions & actual ratings  
‚úî track prompt version (A / B / C)  
‚úî compute accuracy metrics  
‚úî aggregate statistics  
‚úî capture mismatch examples  

This enables **system-level model evaluation** instead of manual testing.

API Source  
`/api/main.py`

Run the API:


Open Swagger UI:   http://127.0.0.1:8000/docs


### API Endpoints

| Endpoint | Description |
|--------|---------|
| `/submit-feedback` | Stores review text, predicted stars, actual stars & explanation |
| `/summary` | Generates accuracy metrics & prompt-wise statistics |

Summary output includes:

‚Ä¢ total entries per prompt  
‚Ä¢ correct vs incorrect predictions  
‚Ä¢ accuracy percentage  
‚Ä¢ star-rating distribution  
‚Ä¢ real example cases  

Summary is exported to:

`/data/evaluation_summary.json`

This creates a **reproducible evaluation dataset**.

---

## Evaluation Metrics & Insights

The system compares:

 Prompt A  
 Prompt B  
 Prompt C  

For each version the API reports:

‚úî total evaluated reviews  
‚úî number of correct predictions  
‚úî incorrect predictions  
‚úî accuracy score (%)  
‚úî behaviour characteristics  
‚úî failure-case examples  

This allowed analysis of:

‚Ä¢ which prompt aligns best with true ratings  
‚Ä¢ which one over-predicts or under-predicts  
‚Ä¢ how tone interpretation affects rating assignment  
‚Ä¢ reasoning consistency vs numerical accuracy  

The results guided final prompt selection.

---

##  Final Project Report

The written report contains:

‚Ä¢ problem understanding  
‚Ä¢ system workflow  
‚Ä¢ experiment design  
‚Ä¢ prompt behaviour study  
‚Ä¢ evaluation methodology  
‚Ä¢ accuracy comparison  
‚Ä¢ observations & conclusions  

üìÑ Full Report  
`/report/Fynd_Report_Vinshee.pdf`

---

##  Tech Stack

‚Ä¢ Python  
‚Ä¢ FastAPI  
‚Ä¢ Gemini LLM API  
‚Ä¢ JSON logging  
‚Ä¢ Evaluation analytics  

---

##  Author

**Vinshee Kulshreshtha**

---

##  Project Outcome

This project demonstrates:

‚úî applied prompt engineering  
‚úî real-world LLM evaluation methodology  
‚úî structured backend logging  
‚úî system-driven accuracy benchmarking  
‚úî research-style analysis approach  

It reflects the ability to design, test,
measure and reason about LLM behaviour ‚Äî beyond simple model usage.



